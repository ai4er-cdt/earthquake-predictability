{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN - p4679 Full Dataset\n",
    "\n",
    "Objectives\n",
    "\n",
    "- Extract full p4679 dataset. Note this requires dealing with large files. Load in full file, but only display smoothed data?\n",
    "- Think about pre-processing. Try to keep as much info as possible, while filtering out noise.. how?\n",
    "- Create train/test split\n",
    "- Create standardised way to do feature creation. Create features for shear stress, derivative and variance.\n",
    "- Create simple LSTM and TCN models\n",
    "- Create information training loop\n",
    "- Create test procedure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Directories\n",
    "\n",
    "jasmin = True  # Set to True if running on JASMIN, False if on local machine\n",
    "jasmin_user_name = \"trr26\"\n",
    "\n",
    "if jasmin:\n",
    "    MAIN_DIR = f\"/gws/nopw/j04/ai4er/users/{jasmin_user_name}/earthquake-predictability\"\n",
    "    DATA_DIR = f\"{MAIN_DIR}/data/gtc_quakes_data\"\n",
    "\n",
    "else:  # update directory names to match your local machine\n",
    "    MAIN_DIR = f\"/home/tom-ratsakatika/VSCode/earthquake-predictability\"\n",
    "    DATA_DIR = f\"{MAIN_DIR}/data_local\"\n",
    "\n",
    "p4679_FILE_PATH = f\"{DATA_DIR}/labquakes/Marone/p4679/p4679.txt\"\n",
    "p4581_FILE_PATH = f\"{DATA_DIR}/labquakes/Marone/p4581/p4581.txt\"\n",
    "\n",
    "# Imports\n",
    "\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from math import sqrt\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from IPython.display import Image, display\n",
    "from scipy.io import loadmat\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(MAIN_DIR)\n",
    "import utils\n",
    "\n",
    "# group utils\n",
    "\n",
    "from utils.general_functions import set_seed, set_torch_device\n",
    "# Set random seed\n",
    "SEED = 42 # random seed for the dataset and model\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check CUDA Availability\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda\"\n",
    "    map_location = None\n",
    "    print(f\"Total number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "    map_location = \"cpu\"\n",
    "    print(\"No GPU available.\")\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(exp):\n",
    "\n",
    "    df_p4679 = pd.DataFrame()\n",
    "    df_p4581 = pd.DataFrame()\n",
    "\n",
    "    if \"p4679\" in exp:\n",
    "        # Open the file located at p4679_FILE_PATH in read mode\n",
    "        with open(p4679_FILE_PATH, \"r\") as file:\n",
    "            # Read the file as a CSV using pandas, considering whitespace as delimiter\n",
    "            # Skip the first 4 rows as they do not contain relevant data\n",
    "            df_p4679 = pd.read_csv(file, skiprows=1)\n",
    "\n",
    "        # Rename the columns of the DataFrame to align with the variable names used by Adriano's data loader\n",
    "        df_p4679.columns = [\n",
    "            \"id\",\n",
    "            \"lp_disp\",\n",
    "            \"shr_stress\",\n",
    "            \"nor_disp\",\n",
    "            \"nor_stress\",\n",
    "            \"time\",\n",
    "            \"mu\",\n",
    "            \"layer_thick\",\n",
    "            \"ec_disp\",\n",
    "        ]\n",
    "\n",
    "        # Drop columns not needed for further analysis\n",
    "        df_p4679 = df_p4679.drop(\n",
    "            [\n",
    "                \"id\",\n",
    "                \"lp_disp\",\n",
    "                \"nor_disp\",\n",
    "                \"mu\",\n",
    "                \"nor_stress\",\n",
    "                \"layer_thick\",\n",
    "                \"ec_disp\",\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # Reorder the DataFrame columns to bring 'time' to the front\n",
    "        df_p4679 = df_p4679[[\"time\"] + [col for col in df_p4679.columns if col != \"time\"]]\n",
    "\n",
    "        # Define start and end times for the data selection\n",
    "        start_time = 4233.364\n",
    "        end_time = 5159.292\n",
    "\n",
    "        df_p4679 = df_p4679[(df_p4679[\"time\"] >= start_time) & (df_p4679[\"time\"] <= end_time)]\n",
    "        df_p4679 = df_p4679.reset_index(drop=True)\n",
    "\n",
    "        # Display the first 5 rows of the DataFrame\n",
    "        print(df_p4679.head())\n",
    "        print(df_p4679.shape)\n",
    "    \n",
    "    if \"p4581\" in exp:\n",
    "\n",
    "        # Open the file located at p4581_FILE_PATH in read mode\n",
    "        with open(p4581_FILE_PATH, \"r\") as file:\n",
    "            # Read the file as a CSV using pandas, considering whitespace as delimiter\n",
    "            # Skip the first 4 rows as they do not contain relevant data\n",
    "            df_p4581 = pd.read_csv(file, delim_whitespace=True, skiprows=4)\n",
    "\n",
    "        # Rename the columns of the DataFrame to align with the variable names used by Adriano's data loader\n",
    "        df_p4581.columns = [\n",
    "            \"id\",            # Identifier\n",
    "            \"lp_disp\",       # Lateral displacement\n",
    "            \"shr_stress\",    # Shear stress\n",
    "            \"nor_disp\",      # Normal displacement\n",
    "            \"nor_stress\",    # Normal stress\n",
    "            \"time\",          # Time of measurement\n",
    "            \"sync\",          # Synchronization marker\n",
    "            \"samp_freq\",     # Sampling frequency\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Drop columns not needed for further analysis\n",
    "        df_p4581 = df_p4581.drop(\n",
    "            [\n",
    "                \"id\",\n",
    "                \"lp_disp\",\n",
    "                \"nor_disp\",\n",
    "                \"nor_stress\",\n",
    "                \"sync\",\n",
    "                \"samp_freq\",\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # Reorder the DataFrame columns to bring 'time' to the front\n",
    "        df_p4581 = df_p4581[[\"time\"] + [col for col in df_p4581.columns if col != \"time\"]]\n",
    "\n",
    "        # Define start and end times for the data selection\n",
    "        start_time = 2100\n",
    "        end_time = 4800\n",
    "\n",
    "        df_p4581 = df_p4581[(df_p4581[\"time\"] >= start_time) & (df_p4581[\"time\"] <= end_time)]\n",
    "        df_p4581 = df_p4581.reset_index(drop=True)\n",
    "\n",
    "        # Display the first 5 rows of the DataFrame\n",
    "        print(df_p4581.head())\n",
    "        print(df_p4581.shape)\n",
    " \n",
    "    df = pd.concat([df_p4679, df_p4581], axis=0)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from one or multiple experiments\n",
    "\n",
    "exp = \"p4679\"       #[\"p4679\", \"p4581\"]\n",
    "df = data_loader(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peak_indices(data, threshold = 500): # 1000 Hz data, so threshold = 200 is 0.2 seconds \n",
    "    peak_indices = [0]\n",
    "    for i in range(threshold, len(data) - threshold):\n",
    "        if data[i] > max(data[i - threshold : i]) and data[i] >= max(\n",
    "            data[i + 1 : i + threshold]\n",
    "        ):\n",
    "            peak_indices.append(i)\n",
    "    return peak_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the find_peak_indices function to get the peak and trough indices - takes ~1 minute to run on sci servers\n",
    "peak_indices = find_peak_indices(df[\"shr_stress\"])\n",
    "trough_indices = find_peak_indices(-df[\"shr_stress\"])\n",
    "\n",
    "df[\"peaks\"] = np.where(df.index.isin(peak_indices), df[\"shr_stress\"], np.nan)\n",
    "df[\"troughs\"] = np.where(\n",
    "    df.index.isin(trough_indices), df[\"shr_stress\"], np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plots the shear stress time series with peaks and troughs marked by crosses (red and green)\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "source = ColumnDataSource(df)\n",
    "\n",
    "p = figure(\n",
    "    width=1200,\n",
    "    height=400,\n",
    "    x_axis_label=\"Time\",\n",
    "    y_axis_label=\"Shear Stress\",\n",
    "    title=\"Shear Stress with Peaks and Troughs Marked\",\n",
    ")\n",
    "p.line(x=\"time\", y=\"shr_stress\", source=source)\n",
    "\n",
    "\n",
    "# Add peaks with red x marks\n",
    "p.cross(x=\"time\", y=\"peaks\", source=source, color=\"red\", size=8)\n",
    "\n",
    "# Add troughs with green x marks\n",
    "p.cross(x=\"time\", y=\"troughs\", source=source, color=\"green\", size=8)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes about 3 mins to run on the sci servers\n",
    "\n",
    "# Calculate time since last peak\n",
    "last_peak_time = None\n",
    "for i in range(len(df)):\n",
    "    if pd.notna(df.loc[i, \"peaks\"]):\n",
    "        last_peak_time = df.loc[i, \"time\"]\n",
    "    df.loc[i, \"time_since_last_peak\"] = (\n",
    "        np.nan\n",
    "        if last_peak_time is None\n",
    "        else df.loc[i, \"time\"] - last_peak_time\n",
    "    )\n",
    "\n",
    "# Calculate time since last trough\n",
    "last_trough_time = None\n",
    "for i in range(len(df)):\n",
    "    if pd.notna(df.loc[i, \"troughs\"]):\n",
    "        last_trough_time = df.loc[i, \"time\"]\n",
    "    df.loc[i, \"time_since_last_trough\"] = (\n",
    "        np.nan\n",
    "        if last_trough_time is None\n",
    "        else df.loc[i, \"time\"] - last_trough_time\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to visualise data. Set visualise_data to True if you want to create plots of variance,\n",
    "# first/second derivative, time sine failure and moving average.\n",
    "# Takes about 1 minute to run on sci servers.\n",
    "\n",
    "visualise_data = True\n",
    "calculate_features = True\n",
    "\n",
    "if calculate_features:\n",
    "\n",
    "        # Calculate moving average of shear stress\n",
    "    df[\"shr_stress_mv_avg\"] = (\n",
    "        df[\"shr_stress\"]\n",
    "        .rolling(100)\n",
    "        .apply(lambda w: scipy.stats.trim_mean(w, 0.05))\n",
    "    )\n",
    "\n",
    "    # Calculate variance of shear stress\n",
    "    df[\"variance\"] = df[\"shr_stress\"].rolling(window=30).var()\n",
    "\n",
    "    # Calculate first derivative of shear stress\n",
    "    df[\"first_derivative\"] = df[\"shr_stress_mv_avg\"].diff()\n",
    "\n",
    "    # Calculate second derivative of shear stress\n",
    "    df[\"second_derivative\"] = df[\"first_derivative\"].diff()\n",
    "\n",
    "\n",
    "if visualise_data:\n",
    "\n",
    "    # Plots time since last peak and time since last trough\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    p = figure(\n",
    "        width=1200,\n",
    "        height=400,\n",
    "        x_axis_label=\"Time\",\n",
    "        y_axis_label=\"Shear Stress\",\n",
    "        title=\"Line Graph of Shear Stress with Peaks and Troughs Marked\",\n",
    "    )\n",
    "    p.line(x=\"time\", y=\"time_since_last_peak\", source=source, line_color=\"red\")\n",
    "    p.line(x=\"time\", y=\"time_since_last_trough\", source=source, line_color=\"green\")\n",
    "\n",
    "    show(p)\n",
    "\n",
    "    # Plots shear stress vs shear stress (moving average)\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    p = figure(\n",
    "        width=1200,\n",
    "        height=400,\n",
    "        x_axis_label=\"Time\",\n",
    "        y_axis_label=\"Shear Stress\",\n",
    "        title=\"shr_stress_mv_avg\",\n",
    "    )\n",
    "    p.line(x=\"time\", y=\"shr_stress\", source=source, line_color=\"blue\")\n",
    "    p.line(x=\"time\", y=\"shr_stress_mv_avg\", source=source, line_color=\"green\")\n",
    "\n",
    "    show(p)\n",
    "\n",
    "    # Plots variance of shear stress (raw), with peaks and troughs marked on x-axis\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    p = figure(\n",
    "        width=1200,\n",
    "        height=400,\n",
    "        x_axis_label=\"Time\",\n",
    "        y_axis_label=\"Shear Stress\",\n",
    "        title=\"variance\",\n",
    "    )\n",
    "\n",
    "    # Primary y-axis for shear stress\n",
    "    p.line(x=\"time\", y=\"variance\", source=source, line_color=\"blue\")\n",
    "\n",
    "    # Add crosses on the x-axis at each index in peak_indices\n",
    "    p.cross(x=df.loc[peak_indices, \"time\"], y=0, size=10, color=\"red\")\n",
    "\n",
    "    # Add crosses on the x-axis at each index in peak_indices\n",
    "    p.cross(x=df.loc[trough_indices, \"time\"], y=0, size=10, color=\"green\")\n",
    "\n",
    "    show(p)\n",
    "\n",
    "    # Plots first derivative of shear stress (moving average), with peaks and troughs marked on x-axis\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    p = figure(\n",
    "        width=1200,\n",
    "        height=400,\n",
    "        x_axis_label=\"Time\",\n",
    "        y_axis_label=\"Shear Stress\",\n",
    "        title=\"first_derivative\",\n",
    "    )\n",
    "\n",
    "    # Primary y-axis for shear stress\n",
    "    p.line(x=\"time\", y=\"first_derivative\", source=source, line_color=\"blue\")\n",
    "\n",
    "    # Add crosses on the x-axis at each index in peak_indices\n",
    "    p.cross(x=df.loc[peak_indices, \"time\"], y=0, size=10, color=\"red\")\n",
    "\n",
    "    # Add crosses on the x-axis at each index in peak_indices\n",
    "    p.cross(x=df.loc[trough_indices, \"time\"], y=0, size=10, color=\"green\")\n",
    "\n",
    "    show(p)\n",
    "\n",
    "    # Plots second derivative of shear stress (moving average), with peaks and troughs marked on x-axis\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    p = figure(\n",
    "        width=1200,\n",
    "        height=400,\n",
    "        x_axis_label=\"Time\",\n",
    "        y_axis_label=\"Shear Stress\",\n",
    "        title=\"second_derivative\",\n",
    "    )\n",
    "\n",
    "    # Primary y-axis for shear stress\n",
    "    p.line(x=\"time\", y=\"second_derivative\", source=source, line_color=\"blue\")\n",
    "\n",
    "    # Add crosses on the x-axis at each index in peak_indices\n",
    "    p.cross(x=df.loc[peak_indices, \"time\"], y=0, size=10, color=\"red\")\n",
    "\n",
    "    # Add crosses on the x-axis at each index in peak_indices\n",
    "    p.cross(x=df.loc[trough_indices, \"time\"], y=0, size=10, color=\"green\")\n",
    "\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the find_peak_indices function to get the peak and trough indices - takes ~1 minute to run on sci servers\n",
    "\n",
    "max_velocity_indices = find_peak_indices(-df[\"first_derivative\"], threshold = 1000)\n",
    "\n",
    "df[\"max_velocity\"] = np.where(df.index.isin(peak_indices), df[\"first_derivative\"], np.nan)\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Plots the shear stress time series with peaks, troughs, and max velocity indices marked by crosses (red, green, and black)\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "source = ColumnDataSource(df)\n",
    "\n",
    "p = figure(\n",
    "    width=1200,\n",
    "    height=400,\n",
    "    x_axis_label=\"Time\",\n",
    "    y_axis_label=\"Shear Stress\",\n",
    "    title=\"Shear Stress with Peaks, Troughs, and Max Velocity Indices Marked\",\n",
    ")\n",
    "p.line(x=\"time\", y=\"shr_stress\", source=source)\n",
    "\n",
    "# Add peaks with red x marks\n",
    "p.cross(x=\"time\", y=\"peaks\", source=source, color=\"red\", size=8)\n",
    "\n",
    "# Add troughs with green x marks\n",
    "p.cross(x=\"time\", y=\"troughs\", source=source, color=\"green\", size=8)\n",
    "\n",
    "# Add max velocity indices with black x marks\n",
    "p.cross(x=df.loc[max_velocity_indices, \"time\"], y=df.loc[max_velocity_indices, \"shr_stress\"], color=\"black\", size=8)\n",
    "\n",
    "show(p)\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "source = ColumnDataSource(df)\n",
    "\n",
    "p = figure(\n",
    "    width=1200,\n",
    "    height=400,\n",
    "    x_axis_label=\"Time\",\n",
    "    y_axis_label=\"Velocity\",\n",
    "    title=\"Max Velocity Indices Marked\",\n",
    ")\n",
    "p.line(x=\"time\", y=\"first_derivative\", source=source)\n",
    "\n",
    "# Add peaks with red x marks\n",
    "p.cross(x=\"time\", y=\"max_velocity\", source=source, color=\"black\", size=8)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segments = []\n",
    "\n",
    "# Reorder the DataFrame columns to bring 'time since last peak/trough' to the front\n",
    "df = df[[\"time_since_last_peak\"] + [col for col in df.columns if col != \"time_since_last_peak\"]]\n",
    "df = df[[\"time_since_last_trough\"] + [col for col in df.columns if col != \"time_since_last_trough\"]]\n",
    "\n",
    "df_segments = np.split(df.drop([\"time\", \"peaks\", \"troughs\"], axis=1), trough_indices[1:])\n",
    "df_segments = [segment.reset_index(drop=True) for segment in df_segments]\n",
    "\n",
    "segment_lengths = [len(segment) for segment in df_segments]\n",
    "\n",
    "df_segments[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the segments\n",
    "\n",
    "# Create a dropdown widget to select the segment\n",
    "segment_dropdown = widgets.Dropdown(\n",
    "    options=list(range(len(df_segments))),\n",
    "    description='Segment:',\n",
    "    value=0,\n",
    ")\n",
    "\n",
    "# Define a function to update the plot based on the selected segment\n",
    "def update_plot(segment):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df_segments[segment]['time_since_last_trough'], df_segments[segment]['shr_stress'])\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Shear Stress')\n",
    "    plt.title(f'Segment {segment}')\n",
    "    plt.show()\n",
    "\n",
    "# Register the update_plot function as the event handler for the dropdown widget\n",
    "widgets.interactive(update_plot, segment=segment_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "\n",
    "def create_features(df):\n",
    "\n",
    "    # Calculate moving average of shear stress - takes about 30 seconds to run on sci servers\n",
    "    df[\"shr_stress_mv_avg\"] = (\n",
    "        df[\"shr_stress\"]\n",
    "        .rolling(100)\n",
    "        .apply(lambda w: scipy.stats.trim_mean(w, 0.05))\n",
    "    )\n",
    "\n",
    "    # Calculate variance of shear stress\n",
    "    df[\"variance\"] = df[\"shr_stress\"].rolling(window=30).var()\n",
    "\n",
    "    # Calculate first derivative of shear stress\n",
    "    df[\"first_derivative\"] = df[\"shr_stress_mv_avg\"].diff()\n",
    "\n",
    "    # Calculate second derivative of shear stress\n",
    "    df[\"second_derivative\"] = df[\"first_derivative\"].diff()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add features columns to segments\n",
    "\n",
    "df_segments_features = df_segments\n",
    "\n",
    "df_segments_features = [create_features(segment) for segment in df_segments]\n",
    "\n",
    "df_segments_features[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(df, columns_to_scale=\"shr_stress\"):\n",
    "    if isinstance(columns_to_scale, str):\n",
    "        columns_to_scale = [columns_to_scale]  # Ensure it's a list if a single column name is passed\n",
    "    \n",
    "    for col in columns_to_scale:\n",
    "        # Explicitly ignore NaN values in min and max calculation\n",
    "        min_val = df[col].min(skipna=True)\n",
    "        max_val = df[col].max(skipna=True)\n",
    "        range_val = max_val - min_val\n",
    "        \n",
    "        # Avoid division by zero if all values in a column are the same\n",
    "        if range_val > 0:\n",
    "            # Apply min-max scaling\n",
    "            df[col] = (df[col] - min_val) / range_val\n",
    "        else:\n",
    "            # Handle the case where all values are the same or if the column only contains NaN values\n",
    "            df[col] = 0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log scaling function\n",
    "\n",
    "def log_scale(df, columns_to_scale=\"variance\"):\n",
    "    if isinstance(columns_to_scale, str):\n",
    "        columns_to_scale = [columns_to_scale]  # Ensure it's a list if a single column name is passed\n",
    "    \n",
    "    for col in columns_to_scale:\n",
    "        # Shift values to ensure they are positive if necessary. \n",
    "        # This requires domain knowledge to choose an appropriate shift.\n",
    "        min_val = df[col].min(skipna=True)\n",
    "        \n",
    "        # Calculate the minimum shift to ensure all values are > 0\n",
    "        shift = 0\n",
    "        if min_val <= 0:\n",
    "            shift = abs(min_val) + 1  # Adding 1 to ensure all values are strictly positive\n",
    "            # print(f\"Shifting {col} by {shift}, because min_val = {min_val}\")\n",
    "        # Apply log scaling with shift\n",
    "        df[col] = np.log(df[col] + shift)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log normalisation then min-max scaling\n",
    "\n",
    "df_segments_features_scaled = df_segments_features\n",
    "\n",
    "columns_to_scale = [\"variance\", \"first_derivative\", \"second_derivative\"]\n",
    "\n",
    "df_segments_features_scaled = [log_scale(segment, columns_to_scale) for segment in df_segments_features[:-1]]\n",
    "\n",
    "# Apply min-max scaling\n",
    "\n",
    "columns_to_scale = [\"shr_stress\", \"shr_stress_mv_avg\", \"variance\", \"first_derivative\", \"second_derivative\"]\n",
    "\n",
    "df_segments_features_scaled = [min_max_scale(segment, columns_to_scale) for segment in df_segments_features[:-1]]\n",
    "\n",
    "df_segments_features_scaled[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(df):\n",
    "    max_length = max(len(seg) for seg in df)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "\n",
    "        current_length = len(df[i])\n",
    "\n",
    "        # Ensure original data has 'mask' column set to 1 (indicating real data)\n",
    "        df[i]['mask'] = 1\n",
    "        \n",
    "        additional_rows = max_length - current_length\n",
    "\n",
    "        if additional_rows > 0:\n",
    "\n",
    "            # Create a dataframe with the padding values and mask set to 0 (indicating padded data)\n",
    "            zeros_df = pd.DataFrame(0, index=np.arange(additional_rows), columns=df[i].columns)\n",
    "            zeros_df['mask'] = 0  # Assuming 0 for padded data\n",
    "\n",
    "            # Concatenate the padding dataframe at the beginning of the original dataframe\n",
    "            df[i] = pd.concat([zeros_df, df[i]]).reset_index(drop=True)\n",
    "            \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding so all the segments are the same length, and add a mask column = 1 for real data\n",
    "\n",
    "df_segments_features_scaled_padded = add_padding(df_segments_features_scaled)\n",
    "df_segments_features_scaled_padded[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Xs and ys, then shuffle and split the segments into train/test/val sets\n",
    "\n",
    "def train_val_test_split(df, y_size = 500, val=0.2, test=0.1, seed = 42, target=\"shr_stress_mv_avg\"):\n",
    "\n",
    "    num_segments = len(df)\n",
    "    train = 1 - val - test\n",
    "    last_train_segment_index = round(train * num_segments) - 1\n",
    "    last_val_segment_index = last_train_segment_index + round(val * num_segments)\n",
    "    last_test_segment_index = last_val_segment_index + round(test * num_segments)\n",
    "\n",
    "    y_start = len(df[0]) - y_size\n",
    "    X = [df_i.iloc[:y_start].copy() for df_i in df]\n",
    "    y = [df_i.iloc[y_start:][[target]].copy() for df_i in df]\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test = train_test_split(X, test_size=test, random_state=seed)\n",
    "    X_train, X_val = train_test_split(X_train, test_size=val, random_state=seed)\n",
    "\n",
    "    y_train, y_test = train_test_split(y, test_size=test, random_state=seed)\n",
    "    y_train, y_val = train_test_split(y_train, test_size=val, random_state=seed)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply train_val_test_split to segments\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(df_segments_features_scaled_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dropdown widget to select the segment\n",
    "segment_dropdown = widgets.Dropdown(\n",
    "    options=list(range(len(X_train))),\n",
    "    description='Segment:',\n",
    "    value=0,\n",
    ")\n",
    "\n",
    "# Define a function to update the plot based on the selected segment\n",
    "def update_plot(segment):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X_train[segment]['shr_stress_mv_avg'])\n",
    "    plt.plot(y_train[segment]['shr_stress_mv_avg'])\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Shear Stress')\n",
    "    plt.title(f'Segment {segment}')\n",
    "    plt.show()\n",
    "\n",
    "# Register the update_plot function as the event handler for the dropdown widget\n",
    "widgets.interactive(update_plot, segment=segment_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each DataFrame to a PyTorch tensor and stack them\n",
    "\n",
    "X_train_tensor = torch.stack([torch.tensor(np.array(df_i), dtype=torch.float32) for df_i in X_train])\n",
    "X_val_tensor = torch.stack([torch.tensor(np.array(df_i), dtype=torch.float32) for df_i in X_val])\n",
    "X_test_tensor = torch.stack([torch.tensor(np.array(df_i), dtype=torch.float32) for df_i in X_test])\n",
    "\n",
    "y_train_tensor = torch.stack([torch.tensor(np.array(df_i), dtype=torch.float32) for df_i in y_train])\n",
    "y_val_tensor = torch.stack([torch.tensor(np.array(df_i), dtype=torch.float32) for df_i in y_val])\n",
    "y_test_tensor = torch.stack([torch.tensor(np.array(df_i), dtype=torch.float32) for df_i in y_test])\n",
    "\n",
    "# Print size and datatype of X tensors\n",
    "print(\"X_train_tensor size:\", X_train_tensor.size(), \"dtype:\", X_train_tensor.dtype)\n",
    "print(\"X_val_tensor size:\", X_val_tensor.size(), \"dtype:\", X_val_tensor.dtype)\n",
    "print(\"X_test_tensor size:\", X_test_tensor.size(), \"dtype:\", X_test_tensor.dtype)\n",
    "\n",
    "# Print size and datatype of y tensors\n",
    "print(\"y_train_tensor size:\", y_train_tensor.size(), \"dtype:\", y_train_tensor.dtype)\n",
    "print(\"y_val_tensor size:\", y_val_tensor.size(), \"dtype:\", y_val_tensor.dtype)\n",
    "print(\"y_test_tensor size:\", y_test_tensor.size(), \"dtype:\", y_test_tensor.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "\n",
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "# For building the TCN model\n",
    "from torch.nn.utils import weight_norm  # Weight normalization can be useful in TCN layers\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "\n",
    "\n",
    "\n",
    "# Additional libraries for data manipulation and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if a GPU is available and set PyTorch to use the GPU, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Display the device to confirm PyTorch device configuration\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                                padding=(kernel_size-1) * dilation, dilation=dilation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv1d(x)[:, :, :-self.conv1d.padding[0]]\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, dilation, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           padding=(kernel_size-1) * dilation, dilation=dilation))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           padding=(kernel_size-1) * dilation, dilation=dilation))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.downsample(x)\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.dropout2(x)\n",
    "        return x + res\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = input_size if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers.append(TemporalBlock(in_channels, out_channels, kernel_size,\n",
    "                                        dilation=dilation_size, dropout=dropout))\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.final_layer = nn.Conv1d(num_channels[-1], output_size, 1)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(500)  # Ensuring output sequence length of 500\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tcn(x)\n",
    "        x = self.final_layer(x)\n",
    "        x = self.adaptive_pool(x)  # Adjust sequence length to 500\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Custom RMSE Loss Function\n",
    "\n",
    "def custom_rmse_loss(output, target):\n",
    "    \"\"\"\n",
    "    Custom Root Mean Squared Error loss function.\n",
    "    \n",
    "    Parameters:\n",
    "    - output: The predictions from the model.\n",
    "    - target: The true values.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: The root mean squared error calculated over the values.\n",
    "    \"\"\"\n",
    "    # Ensure all tensors are on the same device (e.g., CPU or GPU)\n",
    "    output, target = output.to(device), target.to(device)\n",
    "    \n",
    "    # Calculate the squared difference between the predictions and true values\n",
    "    mse_loss = torch.mean((output - target) ** 2)\n",
    "    \n",
    "    # Calculate the square root of the mean squared error\n",
    "    rmse_loss = torch.sqrt(mse_loss)\n",
    "    \n",
    "    return rmse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Data Loaders Corrected for Mask Extraction and Feature Transposition\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Extract the mask tensors before transposing the features\n",
    "mask_train_tensor = X_train_tensor[:, :, -1].unsqueeze(-1)  # Extract mask and ensure it has the same dimensions as y tensors\n",
    "mask_val_tensor = X_val_tensor[:, :, -1].unsqueeze(-1)\n",
    "mask_test_tensor = X_test_tensor[:, :, -1].unsqueeze(-1)\n",
    "\n",
    "# Now extract features excluding the last column (mask) and transpose dimensions to match [batch_size, num_channels, sequence_length]\n",
    "X_train_features = X_train_tensor[:, :, :-1].transpose(1, 2)\n",
    "X_val_features = X_val_tensor[:, :, :-1].transpose(1, 2)\n",
    "X_test_features = X_test_tensor[:, :, :-1].transpose(1, 2)\n",
    "\n",
    "# Create TensorDatasets to hold inputs, targets, and masks\n",
    "train_dataset = TensorDataset(X_train_features, y_train_tensor, mask_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_features, y_val_tensor, mask_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_features, y_test_tensor, mask_test_tensor)\n",
    "\n",
    "# DataLoader parameters\n",
    "batch_size = 32\n",
    "shuffle = True  # Shuffle for the training dataset; not necessary for validation/test datasets\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "print(\"Data loaders prepared with adjusted input dimensions and masks for training, validation, and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Define Training and Evaluation Function\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_and_evaluate_model(model, train_loader, val_loader, optimizer, loss_fn, epochs, device):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The model to train and evaluate.\n",
    "    - train_loader: DataLoader for the training data.\n",
    "    - val_loader: DataLoader for the validation data.\n",
    "    - optimizer: Optimizer to use for training.\n",
    "    - loss_fn: Loss function used for training.\n",
    "    - epochs: Number of training epochs.\n",
    "    - device: Device to train on ('cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    min_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for X_batch, y_batch, mask_batch in train_loader:\n",
    "            X_batch, y_batch, mask_batch = X_batch.to(device), y_batch.to(device), mask_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            outputs = model(X_batch)  # Forward pass\n",
    "            loss = custom_rmse_loss(outputs, y_batch)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            for X_batch, y_batch, mask_batch in val_loader:\n",
    "                X_batch, y_batch, mask_batch = X_batch.to(device), y_batch.to(device), mask_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = loss_fn(outputs, y_batch, mask_batch)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        # Early stopping based on validation loss improvement\n",
    "        if avg_val_loss < min_val_loss:\n",
    "            print(f'Validation Loss Improved ({min_val_loss:.4f} ---> {avg_val_loss:.4f})')\n",
    "            min_val_loss = avg_val_loss\n",
    "            # Save model checkpoint if desired\n",
    "            # torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print('Model Saved')\n",
    "        else:\n",
    "            print('No Improvement in Validation Loss')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Model Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, loss_fn):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The PyTorch model to evaluate.\n",
    "    - data_loader: DataLoader for the dataset to evaluate on.\n",
    "    - loss_fn: The loss function used for evaluation.\n",
    "    \n",
    "    Returns:\n",
    "    - average_loss: The average loss over the dataset.\n",
    "    - average_mae: The average mean absolute error over the dataset.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    total_count = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed\n",
    "        for X_batch, y_batch, mask_batch in data_loader:\n",
    "            X_batch, y_batch, mask_batch = X_batch.to(device), y_batch.to(device), mask_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch, mask_batch)\n",
    "            mae = torch.abs(outputs - y_batch) * mask_batch  # Apply mask to MAE calculation\n",
    "            \n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "            total_mae += mae.sum().item()\n",
    "            total_count += mask_batch.sum().item()  # Count only non-padded entries\n",
    "            \n",
    "    average_loss = total_loss / total_count\n",
    "    average_mae = total_mae / total_count\n",
    "    \n",
    "    return average_loss, average_mae\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Initialize Model and Hyperparameters\n",
    "\n",
    "# Note: Ensure that the TCN model class and the custom masked MSE loss function are defined in previous cells.\n",
    "\n",
    "# Initialize the model\n",
    "model = TCN(input_size=7, output_size=1, num_channels=[16, 32, 64], kernel_size=2, dropout=0.2).to(device)\n",
    "\n",
    "\n",
    "# Define the optimizer with a specified learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Specify the number of epochs for training\n",
    "epochs = 1\n",
    "\n",
    "print(\"Model, optimizer, and hyperparameters initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Execute Training\n",
    "\n",
    "# Call the training function with the initialized components.\n",
    "# The training function, model, optimizer, and loss function should already be defined.\n",
    "# Ensure train_loader and val_loader are set up with your training and validation data.\n",
    "\n",
    "# Start training and validation process\n",
    "print(\"Starting training...\")\n",
    "train_and_evaluate_model(model, train_loader, val_loader, optimizer, masked_mse_loss, epochs, device)\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Evaluation and Results Visualization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions_vs_actuals(outputs, targets):\n",
    "    \"\"\"\n",
    "    Plots the predicted vs. actual values.\n",
    "\n",
    "    Parameters:\n",
    "    - outputs: The model's predictions.\n",
    "    - targets: The actual values.\n",
    "    \"\"\"\n",
    "    # Detach predictions and targets from GPU if necessary and convert to numpy\n",
    "    predictions = outputs.detach().cpu().numpy().flatten()\n",
    "    actuals = targets.detach().cpu().numpy().flatten()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(predictions, label='Predicted', linewidth=2)\n",
    "    plt.plot(actuals, label='Actual', linewidth=2)\n",
    "    plt.title('Predictions vs. Actual Values')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_mae = evaluate_model(model, test_loader, masked_mse_loss)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}')\n",
    "\n",
    "\n",
    "# Visualize predictions for the first batch from the test set\n",
    "model.eval()  # Ensure model is in evaluation mode\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch, mask_batch in test_loader:\n",
    "        X_batch, y_batch, mask_batch = X_batch.to(device), y_batch.to(device), mask_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        # Select the first sequence for visualization\n",
    "        plot_predictions_vs_actuals(outputs[0], y_batch[0])\n",
    "        break  # Only visualize the first batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "\n",
    "- Convert dataframes into tensors\n",
    "- Create a data loader\n",
    "- Create a model that takes into account the mask variable\n",
    "- Just try and predict the moving average\n",
    "- Bring in data from p4581\n",
    "- Maybe downsample - so predicting out 50 steps instead of 500 and include other experiments - but first see if something useful in high res data\n",
    "- How to utilise GPUs - look up vptop = it's a task manager, sees cpus memory etc.\n",
    "- Look into CNN / FNO - could be useful for long term trends\n",
    "- Look into clustering peaks to determine similaritiy\n",
    "- Look at gausian process - inital kernel would be a sin wave.. and would be qutie efficient with little data\n",
    "\n",
    "Other features\n",
    "- \n",
    "\n",
    "Statistics to look at\n",
    "- From red star to peak of velocity (i.e. min of derivvative) - have a recursive map.. the next time between the two and hte previous one..\n",
    "- Lorenz map? To go from a high dimensional system to a 1D map - a function that brings you from a one value to the next - discrete map\n",
    "- Plot maxima vs previosu maxima\n",
    "- Add other past stats as constants - amplitude and cycle length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN vs LSTM - Using Full p4679 Dataset\n",
    "\n",
    "Objectives\n",
    "\n",
    "- Extract full p4679 dataset. Note this requires dealing with large files. Load in full file, but only display smoothed data?\n",
    "- Think about pre-processing. Try to keep as much info as possible, while filtering out noise.. how?\n",
    "- Create train/test split\n",
    "- Create standardised way to do feature creation. Create features for shear stress, derivative and variance.\n",
    "- Create simple LSTM and TCN models\n",
    "- Create information training loop\n",
    "- Create test procedure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Directories\n",
    "\n",
    "jasmin = True  # Set to True if running on JASMIN, False if on local machine\n",
    "jasmin_user_name = \"trr26\"\n",
    "\n",
    "if jasmin:\n",
    "    MAIN_DIR = f\"/gws/nopw/j04/ai4er/users/{jasmin_user_name}/earthquake-predictability\"\n",
    "    DATA_DIR = f\"{MAIN_DIR}/data/gtc_quakes_data\"\n",
    "\n",
    "else:  # update directory names to match your local machine\n",
    "    MAIN_DIR = f\"/home/tom-ratsakatika/VSCode/earthquake-predictability\"\n",
    "    DATA_DIR = f\"{MAIN_DIR}/data_local\"\n",
    "\n",
    "p4679_FILE_PATH = f\"{DATA_DIR}/labquakes/Marone/p4679/p4679.txt\"\n",
    "\n",
    "# Imports\n",
    "\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from math import sqrt\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from IPython.display import Image, display\n",
    "from scipy.io import loadmat\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(MAIN_DIR)\n",
    "import utils\n",
    "\n",
    "# Check CUDA Availability\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda\"\n",
    "    map_location = None\n",
    "    print(f\"Total number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "    map_location = \"cpu\"\n",
    "    print(\"No GPU available.\")\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file located at p4581_FILE_PATH in read mode\n",
    "with open(p4679_FILE_PATH, \"r\") as file:\n",
    "    # Read the file as a CSV using pandas, considering whitespace as delimiter\n",
    "    # Skip the first 4 rows as they do not contain relevant data\n",
    "    df = pd.read_csv(file, skiprows=1)\n",
    "\n",
    "# Rename the columns of the DataFrame to align with the variable names used by Adriano's data loader\n",
    "df.columns = [\n",
    "    \"id\",\n",
    "    \"lp_disp\",\n",
    "    \"shr_stress\",\n",
    "    \"nor_disp\",\n",
    "    \"nor_stress\",\n",
    "    \"time\",\n",
    "    \"mu\",\n",
    "    \"layer_thick\",\n",
    "    \"ec_disp\",\n",
    "]\n",
    "\n",
    "# Drop columns not needed for further analysis\n",
    "df = df.drop(\n",
    "    [\n",
    "        \"id\",\n",
    "        \"lp_disp\",\n",
    "        \"nor_disp\",\n",
    "        \"mu\",\n",
    "        \"nor_stress\",\n",
    "        \"layer_thick\",\n",
    "        \"ec_disp\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Reorder the DataFrame columns to bring 'time' to the front\n",
    "df = df[[\"time\"] + [col for col in df.columns if col != \"time\"]]\n",
    "\n",
    "# Define start and end times for the data selection\n",
    "start_time = 4233.364\n",
    "end_time = 5159.292\n",
    "\n",
    "df = df[(df[\"time\"] >= start_time) & (df[\"time\"] <= end_time)]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peak_indices(data, threshold = 200): # 1000 Hz data, so threshold = 200 is 0.2 seconds \n",
    "    peak_indices = [0]\n",
    "    for i in range(threshold, len(data) - threshold):\n",
    "        if data[i] > max(data[i - threshold : i]) and data[i] >= max(\n",
    "            data[i + 1 : i + threshold]\n",
    "        ):\n",
    "            peak_indices.append(i)\n",
    "    return peak_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the find_peak_indices function to get the peak and trough indices - takes ~1 minute to run on sci servers\n",
    "peak_indices = find_peak_indices(df[\"shr_stress\"])\n",
    "trough_indices = find_peak_indices(-df[\"shr_stress\"])\n",
    "\n",
    "df[\"peaks\"] = np.where(df.index.isin(peak_indices), df[\"shr_stress\"], np.nan)\n",
    "df[\"troughs\"] = np.where(\n",
    "    df.index.isin(trough_indices), df[\"shr_stress\"], np.nan)\n",
    "\n",
    "# Plots the shear stress time series with peaks and troughs marked by crosses (red and green)\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "source = ColumnDataSource(df)\n",
    "\n",
    "p = figure(\n",
    "    width=1200,\n",
    "    height=400,\n",
    "    x_axis_label=\"Time\",\n",
    "    y_axis_label=\"Shear Stress\",\n",
    "    title=\"Shear Stress with Peaks and Troughs Marked\",\n",
    ")\n",
    "p.line(x=\"time\", y=\"shr_stress\", source=source)\n",
    "\n",
    "\n",
    "# Add peaks with red x marks\n",
    "p.cross(x=\"time\", y=\"peaks\", source=source, color=\"red\", size=8)\n",
    "\n",
    "# Add troughs with green x marks\n",
    "p.cross(x=\"time\", y=\"troughs\", source=source, color=\"green\", size=8)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes about 3 mins to run on the sci servers\n",
    "\n",
    "# Calculate time since last peak\n",
    "last_peak_time = None\n",
    "for i in range(len(df)):\n",
    "    if pd.notna(df.loc[i, \"peaks\"]):\n",
    "        last_peak_time = df.loc[i, \"time\"]\n",
    "    df.loc[i, \"time_since_last_peak\"] = (\n",
    "        np.nan\n",
    "        if last_peak_time is None\n",
    "        else df.loc[i, \"time\"] - last_peak_time\n",
    "    )\n",
    "\n",
    "# Calculate time since last trough\n",
    "last_trough_time = None\n",
    "for i in range(len(df)):\n",
    "    if pd.notna(df.loc[i, \"troughs\"]):\n",
    "        last_trough_time = df.loc[i, \"time\"]\n",
    "    df.loc[i, \"time_since_last_trough\"] = (\n",
    "        np.nan\n",
    "        if last_trough_time is None\n",
    "        else df.loc[i, \"time\"] - last_trough_time\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to visualise data. Set visualise_data to True if you want to create plots of variance,\n",
    "# first/second derivative, time sine failure and moving average.\n",
    "# Takes about 1 minute to run on sci servers.\n",
    "\n",
    "visualise_data = False\n",
    "\n",
    "if visualise_data:\n",
    "\n",
    "    # Calculate moving average of shear stress\n",
    "    df[\"shr_stress_mv_avg\"] = (\n",
    "        df[\"shr_stress\"]\n",
    "        .rolling(100)\n",
    "        .apply(lambda w: scipy.stats.trim_mean(w, 0.05))\n",
    "    )\n",
    "\n",
    "    # Calculate variance of shear stress\n",
    "    df[\"variance\"] = df[\"shr_stress\"].rolling(window=30).var()\n",
    "\n",
    "    # Calculate first derivative of shear stress\n",
    "    df[\"first_derivative\"] = df[\"shr_stress_mv_avg\"].diff()\n",
    "\n",
    "    # Calculate second derivative of shear stress\n",
    "    df[\"second_derivative\"] = df[\"first_derivative\"].diff()\n",
    "\n",
    "\n",
    "    # Plots time since last peak and time since last trough\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    p = figure(\n",
    "        width=1200,\n",
    "        height=400,\n",
    "        x_axis_label=\"Time\",\n",
    "        y_axis_label=\"Shear Stress\",\n",
    "        title=\"Line Graph of Shear Stress with Peaks and Troughs Marked\",\n",
    "    )\n",
    "    p.line(x=\"time\", y=\"time_since_last_peak\", source=source, line_color=\"red\")\n",
    "    p.line(x=\"time\", y=\"time_since_last_trough\", source=source, line_color=\"green\")\n",
    "\n",
    "    show(p)\n",
    "\n",
    "    # Plots shear stress vs shear stress (moving average)\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    p = figure(\n",
    "        width=1200,\n",
    "        height=400,\n",
    "        x_axis_label=\"Time\",\n",
    "        y_axis_label=\"Shear Stress\",\n",
    "        title=\"shr_stress_mv_avg\",\n",
    "    )\n",
    "    p.line(x=\"time\", y=\"shr_stress\", source=source, line_color=\"blue\")\n",
    "    p.line(x=\"time\", y=\"shr_stress_mv_avg\", source=source, line_color=\"green\")\n",
    "\n",
    "    show(p)\n",
    "\n",
    "    # Plots variance of shear stress (raw), with peaks and troughs marked on x-axis\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    p = figure(\n",
    "        width=1200,\n",
    "        height=400,\n",
    "        x_axis_label=\"Time\",\n",
    "        y_axis_label=\"Shear Stress\",\n",
    "        title=\"variance\",\n",
    "    )\n",
    "\n",
    "    # Primary y-axis for shear stress\n",
    "    p.line(x=\"time\", y=\"variance\", source=source, line_color=\"blue\")\n",
    "\n",
    "    # Add crosses on the x-axis at each index in peak_indices\n",
    "    p.cross(x=df.loc[peak_indices, \"time\"], y=0, size=10, color=\"red\")\n",
    "\n",
    "    # Add crosses on the x-axis at each index in peak_indices\n",
    "    p.cross(x=df.loc[trough_indices, \"time\"], y=0, size=10, color=\"green\")\n",
    "\n",
    "    show(p)\n",
    "\n",
    "    # Plots first derivative of shear stress (moving average), with peaks and troughs marked on x-axis\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    p = figure(\n",
    "        width=1200,\n",
    "        height=400,\n",
    "        x_axis_label=\"Time\",\n",
    "        y_axis_label=\"Shear Stress\",\n",
    "        title=\"first_derivative\",\n",
    "    )\n",
    "\n",
    "    # Primary y-axis for shear stress\n",
    "    p.line(x=\"time\", y=\"first_derivative\", source=source, line_color=\"blue\")\n",
    "\n",
    "    # Add crosses on the x-axis at each index in peak_indices\n",
    "    p.cross(x=df.loc[peak_indices, \"time\"], y=0, size=10, color=\"red\")\n",
    "\n",
    "    # Add crosses on the x-axis at each index in peak_indices\n",
    "    p.cross(x=df.loc[trough_indices, \"time\"], y=0, size=10, color=\"green\")\n",
    "\n",
    "    show(p)\n",
    "\n",
    "    # Plots second derivative of shear stress (moving average), with peaks and troughs marked on x-axis\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    p = figure(\n",
    "        width=1200,\n",
    "        height=400,\n",
    "        x_axis_label=\"Time\",\n",
    "        y_axis_label=\"Shear Stress\",\n",
    "        title=\"second_derivative\",\n",
    "    )\n",
    "\n",
    "    # Primary y-axis for shear stress\n",
    "    p.line(x=\"time\", y=\"second_derivative\", source=source, line_color=\"blue\")\n",
    "\n",
    "    # Add crosses on the x-axis at each index in peak_indices\n",
    "    p.cross(x=df.loc[peak_indices, \"time\"], y=0, size=10, color=\"red\")\n",
    "\n",
    "    # Add crosses on the x-axis at each index in peak_indices\n",
    "    p.cross(x=df.loc[trough_indices, \"time\"], y=0, size=10, color=\"green\")\n",
    "\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segments = []\n",
    "\n",
    "# Reorder the DataFrame columns to bring 'time since last peak/trough' to the front\n",
    "df = df[[\"time_since_last_peak\"] + [col for col in df.columns if col != \"time_since_last_peak\"]]\n",
    "df = df[[\"time_since_last_trough\"] + [col for col in df.columns if col != \"time_since_last_trough\"]]\n",
    "\n",
    "df_segments = np.split(df.drop([\"time\", \"peaks\", \"troughs\"], axis=1), trough_indices[1:])\n",
    "df_segments = [segment.reset_index(drop=True) for segment in df_segments]\n",
    "\n",
    "segment_lengths = [len(segment) for segment in df_segments]\n",
    "\n",
    "df_segments[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the segments\n",
    "\n",
    "\n",
    "# Create a dropdown widget to select the segment\n",
    "segment_dropdown = widgets.Dropdown(\n",
    "    options=list(range(len(df_segments))),\n",
    "    description='Segment:',\n",
    "    value=0,\n",
    ")\n",
    "\n",
    "# Define a function to update the plot based on the selected segment\n",
    "def update_plot(segment):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df_segments[segment]['time_since_last_trough'], df_segments[segment]['shr_stress'])\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Shear Stress')\n",
    "    plt.title(f'Segment {segment}')\n",
    "    plt.show()\n",
    "\n",
    "# Register the update_plot function as the event handler for the dropdown widget\n",
    "widgets.interactive(update_plot, segment=segment_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "\n",
    "def create_features(df):\n",
    "\n",
    "    # Calculate moving average of shear stress - takes about 30 seconds to run on sci servers\n",
    "    df[\"shr_stress_mv_avg\"] = (\n",
    "        df[\"shr_stress\"]\n",
    "        .rolling(100)\n",
    "        .apply(lambda w: scipy.stats.trim_mean(w, 0.05))\n",
    "    )\n",
    "\n",
    "    # Calculate variance of shear stress\n",
    "    df[\"variance\"] = df[\"shr_stress\"].rolling(window=30).var()\n",
    "\n",
    "    # Calculate first derivative of shear stress\n",
    "    df[\"first_derivative\"] = df[\"shr_stress_mv_avg\"].diff()\n",
    "\n",
    "    # Calculate second derivative of shear stress\n",
    "    df[\"second_derivative\"] = df[\"first_derivative\"].diff()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add features columns to segments\n",
    "\n",
    "df_segments_features = df_segments\n",
    "\n",
    "df_segments_features = [create_features(segment) for segment in df_segments]\n",
    "\n",
    "df_segments_features[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(df, columns_to_scale=\"shr_stress\"):\n",
    "    if isinstance(columns_to_scale, str):\n",
    "        columns_to_scale = [columns_to_scale]  # Ensure it's a list if a single column name is passed\n",
    "    \n",
    "    for col in columns_to_scale:\n",
    "        # Explicitly ignore NaN values in min and max calculation\n",
    "        min_val = df[col].min(skipna=True)\n",
    "        max_val = df[col].max(skipna=True)\n",
    "        range_val = max_val - min_val\n",
    "        \n",
    "        # Avoid division by zero if all values in a column are the same\n",
    "        if range_val > 0:\n",
    "            # Apply min-max scaling\n",
    "            df[col] = (df[col] - min_val) / range_val\n",
    "        else:\n",
    "            # Handle the case where all values are the same or if the column only contains NaN values\n",
    "            df[col] = 0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply min-max scaling\n",
    "\n",
    "df_segments_features_scaled = df_segments_features\n",
    "\n",
    "columns_to_scale = [\"shr_stress\", \"shr_stress_mv_avg\", \"variance\", \"first_derivative\", \"second_derivative\"]\n",
    "\n",
    "df_segments_features_scaled = [min_max_scale(segment, columns_to_scale) for segment in df_segments_features[:-1]]\n",
    "\n",
    "df_segments_features_scaled[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(df):\n",
    "    max_length = max(len(seg) for seg in df)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "\n",
    "        current_length = len(df[i])\n",
    "\n",
    "        if current_length < max_length:\n",
    "            \n",
    "            additional_rows = max_length - current_length\n",
    "\n",
    "            # Create a dataframe with the padding values and mask set to 0 (indicating padded data)\n",
    "            zeros_df = pd.DataFrame(0, index=np.arange(additional_rows), columns=df[i].columns)\n",
    "            zeros_df['mask'] = 0  # Assuming 0 for padded data\n",
    "            \n",
    "            # Ensure original data has 'mask' column set to 1 (indicating real data)\n",
    "            df[i]['mask'] = 1\n",
    "            \n",
    "            # Concatenate the padding dataframe at the beginning of the original dataframe\n",
    "            df[i] = pd.concat([zeros_df, df[i]]).reset_index(drop=True)\n",
    "            \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding so all the segments are the same length, and add a mask column = 1 for real data\n",
    "\n",
    "df_segments_features_scaled_padded = add_padding(df_segments_features_scaled)\n",
    "df_segments_features_scaled_padded[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Xs and ys, then shuffle and split the segments into train/test/val sets\n",
    "\n",
    "def train_val_test_split(df, y_size = 500, val=0.2, test=0.1, seed = 42):\n",
    "\n",
    "    num_segments = len(df)\n",
    "    train = 1 - val - test\n",
    "    last_train_segment_index = round(train * num_segments) - 1\n",
    "    last_val_segment_index = last_train_segment_index + round(val * num_segments)\n",
    "    last_test_segment_index = last_val_segment_index + round(test * num_segments)\n",
    "\n",
    "    y_start = len(df[0]) - y_size\n",
    "    X = [df_i.iloc[:y_start].copy() for df_i in df]\n",
    "    y = [df_i.iloc[y_start:].copy() for df_i in df]\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test = train_test_split(X, test_size=test, random_state=seed)\n",
    "    X_train, X_val = train_test_split(X_train, test_size=val, random_state=seed)\n",
    "\n",
    "    y_train, y_test = train_test_split(y, test_size=test, random_state=seed)\n",
    "    y_train, y_val = train_test_split(y_train, test_size=val, random_state=seed)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbba02c36efb4b44bba66e4048d2cc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Segment:', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, â€¦"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply train_val_test_split to segments\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(df_segments_features_scaled_padded)\n",
    "\n",
    "# Create a dropdown widget to select the segment\n",
    "segment_dropdown = widgets.Dropdown(\n",
    "    options=list(range(len(X_train))),\n",
    "    description='Segment:',\n",
    "    value=0,\n",
    ")\n",
    "\n",
    "# Define a function to update the plot based on the selected segment\n",
    "def update_plot(segment):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X_train[segment]['shr_stress'])\n",
    "    plt.plot(y_train[segment]['shr_stress'])\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Shear Stress')\n",
    "    plt.title(f'Segment {segment}')\n",
    "    plt.show()\n",
    "\n",
    "# Register the update_plot function as the event handler for the dropdown widget\n",
    "widgets.interactive(update_plot, segment=segment_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "\n",
    "- Convert dataframes into tensors\n",
    "- Create a data loader\n",
    "- Create a model that takes into account the mask variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                               stride=stride, padding=padding, dilation=dilation)\n",
    "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                               stride=stride, padding=padding, dilation=dilation)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.conv2(out))\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.linear = nn.Linear(num_channels[-1], out_steps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # Transpose batch and sequence dims\n",
    "        y = self.tcn(x)\n",
    "        y = y.transpose(1, 2)  # Transpose back\n",
    "        y = self.linear(y[:, -1, :])  # Take the last output for each sequence\n",
    "        return y\n",
    "\n",
    "# Assuming num_features and out_steps are defined\n",
    "num_features = X_train[0].shape[1]\n",
    "out_steps = 500\n",
    "\n",
    "# Convert DataFrames to tensors\n",
    "def df_to_tensor(df_list, feature_columns=None, target_column=None):\n",
    "    features = []\n",
    "    targets = []\n",
    "    for df in df_list:\n",
    "        if feature_columns is not None:\n",
    "            features.append(torch.tensor(df[feature_columns].values, dtype=torch.float32))\n",
    "        if target_column is not None:\n",
    "            targets.append(torch.tensor(df[target_column].values, dtype=torch.float32))\n",
    "    features = torch.stack(features)\n",
    "    targets = torch.stack(targets)\n",
    "    return features, targets\n",
    "\n",
    "# Assuming X_train, y_train, etc. are lists of DataFrames\n",
    "X_train_tensor, _ = df_to_tensor(X_train, feature_columns=X_train[0].columns)\n",
    "_, y_train_tensor = df_to_tensor(y_train, target_column='shr_stress')\n",
    "\n",
    "X_val_tensor, _ = df_to_tensor(X_val, feature_columns=X_val[0].columns)\n",
    "_, y_val_tensor = df_to_tensor(y_val, target_column='shr_stress')\n",
    "\n",
    "X_test_tensor, _ = df_to_tensor(X_test, feature_columns=X_test[0].columns)\n",
    "_, y_test_tensor = df_to_tensor(y_test, target_column='shr_stress')\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "model = TCN(num_inputs=num_features, num_channels=[25, 50, 100])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

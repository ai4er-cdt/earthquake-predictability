{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff22d22",
   "metadata": {},
   "source": [
    "# Data Preprocessing Notes for Cascadia\n",
    "Author: Camilla Billari <br> \n",
    "Date: 17/01/24\n",
    "\n",
    "Notes for the data preprocessing that was carried out upon loading Cascadia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d680b4a8-f747-4ebc-9459-1c5f2d1ea356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "MAIN_DICT = \"/gws/nopw/j04/ai4er/users/pn341/earthquake-predictability\"\n",
    "sys.path.append(MAIN_DICT)\n",
    "\n",
    "from utils.dataset import SlowEarthquakeDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab923818",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ccccb",
   "metadata": {},
   "source": [
    "Quickly opened it in Matlab for a sanity check:\n",
    "\n",
    "<img src=\"/gws/nopw/j04/ai4er/users/cgbill/earthquake-predictability/assets/images/cgb47-notebooks/Datamat.png\" alt=\"Loaded Data in Matlab Screenshot\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3795856b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: ['#refs#', 'Area', 'Area_asp', 'Area_belt', 'Lg_term_slip_Rate', 'Nasp', 'Npatches', 'Npatches_asp', 'Npatches_belt', 'Nt_u', 'Nt_v', 'Upotrough_asp', 'Upotroughdot_asp', 'V0pot_asp', 'Vpotrough_asp', 'aa', 'dirs', 'fault_model_IC', 'flag_surrogate', 'id_IN_AspBorder_SSE', 'id_IN_AspBorder_SSE_dilat', 'ind_in', 'ind_in_belt', 'ind_nan', 'km2m', 'll_geom_center_ll', 'mm2m', 'shear_modulus', 'timeline_u', 'timeline_v', 'toc_start', 'upotrough', 'upotrough_asp', 'upotrough_belt', 'upotroughdot', 'upotroughdot_asp', 'upotroughdot_belt', 'urough', 'urough_asp', 'urough_belt', 'urough_mean', 'uroughdot', 'uroughdot_asp', 'uroughdot_belt', 'v0', 'v0_asp', 'v0_belt', 'v0pot', 'v0pot_asp', 'v0pot_belt', 'vpotrough', 'vpotrough_asp', 'vpotrough_belt', 'vrough', 'vrough_asp', 'vrough_belt']\n",
      "Column types: [<HDF5 group \"/#refs#\" (184 members)>, <HDF5 dataset \"Area\": shape (1, 3337), type \"<f8\">, <HDF5 dataset \"Area_asp\": shape (1, 14), type \"|O\">, <HDF5 dataset \"Area_belt\": shape (1, 557), type \"<f8\">, <HDF5 dataset \"Lg_term_slip_Rate\": shape (2, 3339), type \"<f8\">, <HDF5 dataset \"Nasp\": shape (1, 1), type \"<f8\">, <HDF5 dataset \"Npatches\": shape (1, 1), type \"<f8\">, <HDF5 dataset \"Npatches_asp\": shape (14, 1), type \"<f8\">, <HDF5 dataset \"Npatches_belt\": shape (1, 1), type \"<f8\">, <HDF5 dataset \"Nt_u\": shape (1, 1), type \"<f8\">, <HDF5 dataset \"Nt_v\": shape (1, 1), type \"<f8\">, <HDF5 dataset \"Upotrough_asp\": shape (3883, 14), type \"<f8\">, <HDF5 dataset \"Upotroughdot_asp\": shape (3882, 14), type \"<f8\">, <HDF5 dataset \"V0pot_asp\": shape (3882, 14), type \"<f8\">, <HDF5 dataset \"Vpotrough_asp\": shape (3882, 14), type \"<f8\">, <HDF5 dataset \"aa\": shape (1, 1), type \"<f8\">, <HDF5 group \"/dirs\" (5 members)>, <HDF5 dataset \"fault_model_IC\": shape (15, 1), type \"|O\">, <HDF5 dataset \"flag_surrogate\": shape (1, 1), type \"<f8\">, <HDF5 dataset \"id_IN_AspBorder_SSE\": shape (14, 1), type \"|O\">, <HDF5 dataset \"id_IN_AspBorder_SSE_dilat\": shape (14, 1), type \"|O\">, <HDF5 dataset \"ind_in\": shape (1, 14), type \"|O\">, <HDF5 dataset \"ind_in_belt\": shape (1, 3337), type \"|u1\">, <HDF5 dataset \"ind_nan\": shape (1, 3339), type \"|u1\">, <HDF5 dataset \"km2m\": shape (1, 1), type \"<f8\">, <HDF5 dataset \"ll_geom_center_ll\": shape (2, 3337), type \"<f8\">, <HDF5 dataset \"mm2m\": shape (1, 1), type \"<f8\">, <HDF5 dataset \"shear_modulus\": shape (1, 1), type \"<f8\">, <HDF5 dataset \"timeline_u\": shape (3883, 1), type \"<f8\">, <HDF5 dataset \"timeline_v\": shape (3882, 1), type \"<f8\">, <HDF5 dataset \"toc_start\": shape (1, 1), type \"<f8\">, <HDF5 dataset \"upotrough\": shape (3883, 3337), type \"<f8\">, <HDF5 dataset \"upotrough_asp\": shape (1, 14), type \"|O\">, <HDF5 dataset \"upotrough_belt\": shape (3883, 557), type \"<f8\">, <HDF5 dataset \"upotroughdot\": shape (3882, 3337), type \"<f8\">, <HDF5 dataset \"upotroughdot_asp\": shape (1, 14), type \"|O\">, <HDF5 dataset \"upotroughdot_belt\": shape (3882, 557), type \"<f8\">, <HDF5 dataset \"urough\": shape (3883, 3337), type \"<f8\">, <HDF5 dataset \"urough_asp\": shape (1, 14), type \"|O\">, <HDF5 dataset \"urough_belt\": shape (3883, 557), type \"<f8\">, <HDF5 dataset \"urough_mean\": shape (1, 3339), type \"<f8\">, <HDF5 dataset \"uroughdot\": shape (3882, 3337), type \"<f8\">, <HDF5 dataset \"uroughdot_asp\": shape (1, 14), type \"|O\">, <HDF5 dataset \"uroughdot_belt\": shape (3882, 557), type \"<f8\">, <HDF5 dataset \"v0\": shape (1, 3337), type \"<f8\">, <HDF5 dataset \"v0_asp\": shape (1, 14), type \"|O\">, <HDF5 dataset \"v0_belt\": shape (1, 557), type \"<f8\">, <HDF5 dataset \"v0pot\": shape (1, 3337), type \"<f8\">, <HDF5 dataset \"v0pot_asp\": shape (1, 14), type \"|O\">, <HDF5 dataset \"v0pot_belt\": shape (1, 557), type \"<f8\">, <HDF5 dataset \"vpotrough\": shape (3882, 3337), type \"<f8\">, <HDF5 dataset \"vpotrough_asp\": shape (1, 14), type \"|O\">, <HDF5 dataset \"vpotrough_belt\": shape (3882, 557), type \"<f8\">, <HDF5 dataset \"vrough\": shape (3882, 3337), type \"<f8\">, <HDF5 dataset \"vrough_asp\": shape (1, 14), type \"|O\">, <HDF5 dataset \"vrough_belt\": shape (3882, 557), type \"<f8\">]\n",
      "Number of columns: 56\n"
     ]
    }
   ],
   "source": [
    "# Directories paths\n",
    "GTC_DATA_DIR = \"/gws/nopw/j04/ai4er/users/pn341/earthquake-predictability/data/gtc_quakes_data\"\n",
    "SLOWQUAKES_DATA_DIR = f\"{GTC_DATA_DIR}/slowquakes\"\n",
    "MICHEL_DATA_DIR = f\"{SLOWQUAKES_DATA_DIR}/Micheletal2019a\"\n",
    "\n",
    "# Access the .mat file using h5py which reads it in Hierarchical Data Format (version 5) to preserve references\n",
    "data_f = h5py.File(\n",
    "    f\"{MICHEL_DATA_DIR}/dynamical_system_variables_rough.mat\", \"r\"\n",
    ")\n",
    "keys = list(data_f.keys())\n",
    "values = list(data_f.values())\n",
    "print(\"Column names:\", keys)\n",
    "print(\"Column types:\", values)\n",
    "print(\"Number of columns:\", len(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85934c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data loader accesses \"upotrough_asp\", which is split into the 14 segments, which are triangulated\n",
    "data_f[\"upotrough_asp\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c1cb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of t samples: 3883, n of segments: 14\n"
     ]
    }
   ],
   "source": [
    "# Access the time array\n",
    "t_data = data_f[\"timeline_u\"]\n",
    "t = t_data[()][\n",
    "    :, 0\n",
    "]  # t_data[()] gets entire dataset; then select rows from the first column\n",
    "dt = t[1] - t[0]  # look at time step in data (1 day)\n",
    "\n",
    "n_samples = t.shape[0]  # n of samples (time)\n",
    "n_segments = data_f[\"upotrough_asp\"].shape[\n",
    "    1\n",
    "]  # diplacement potency of asperity (rough??)\n",
    "\n",
    "print(\"N of t samples: {}, n of segments: {}\".format(n_samples, n_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e01ce09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87022.678176</td>\n",
       "      <td>-51216.196044</td>\n",
       "      <td>-195371.505863</td>\n",
       "      <td>-252388.298108</td>\n",
       "      <td>372090.056832</td>\n",
       "      <td>290238.755195</td>\n",
       "      <td>271991.646241</td>\n",
       "      <td>218795.685053</td>\n",
       "      <td>87096.609524</td>\n",
       "      <td>86361.765678</td>\n",
       "      <td>...</td>\n",
       "      <td>-651953.434874</td>\n",
       "      <td>-714770.112748</td>\n",
       "      <td>-631423.743396</td>\n",
       "      <td>-693599.810348</td>\n",
       "      <td>-7.208255e+05</td>\n",
       "      <td>-739527.228854</td>\n",
       "      <td>-471014.225482</td>\n",
       "      <td>-6.757540e+05</td>\n",
       "      <td>-350155.612260</td>\n",
       "      <td>-517024.526987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118857.552212</td>\n",
       "      <td>64765.192887</td>\n",
       "      <td>-66943.170211</td>\n",
       "      <td>-125253.475061</td>\n",
       "      <td>510980.726973</td>\n",
       "      <td>375269.217942</td>\n",
       "      <td>323695.517273</td>\n",
       "      <td>312924.577753</td>\n",
       "      <td>-6911.497818</td>\n",
       "      <td>92492.903441</td>\n",
       "      <td>...</td>\n",
       "      <td>-561081.908424</td>\n",
       "      <td>-506075.873333</td>\n",
       "      <td>-448764.275644</td>\n",
       "      <td>-391456.474306</td>\n",
       "      <td>-9.886575e+05</td>\n",
       "      <td>-958004.348328</td>\n",
       "      <td>-740390.446945</td>\n",
       "      <td>-8.927657e+05</td>\n",
       "      <td>-466337.939160</td>\n",
       "      <td>-928475.702850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-113120.619296</td>\n",
       "      <td>-100329.909243</td>\n",
       "      <td>-117236.092138</td>\n",
       "      <td>-63257.566664</td>\n",
       "      <td>87699.655400</td>\n",
       "      <td>15131.208777</td>\n",
       "      <td>-22576.356489</td>\n",
       "      <td>-13975.055467</td>\n",
       "      <td>-177396.925200</td>\n",
       "      <td>-125223.619520</td>\n",
       "      <td>...</td>\n",
       "      <td>-450443.930378</td>\n",
       "      <td>-421853.932989</td>\n",
       "      <td>-333693.022748</td>\n",
       "      <td>-204519.903296</td>\n",
       "      <td>-7.137062e+05</td>\n",
       "      <td>-715494.785171</td>\n",
       "      <td>-542145.799983</td>\n",
       "      <td>-6.803446e+05</td>\n",
       "      <td>-366915.287037</td>\n",
       "      <td>-572674.592498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-51897.143096</td>\n",
       "      <td>-61146.543996</td>\n",
       "      <td>-108560.424295</td>\n",
       "      <td>-80686.019435</td>\n",
       "      <td>178525.612880</td>\n",
       "      <td>98101.791008</td>\n",
       "      <td>48849.079713</td>\n",
       "      <td>53199.150697</td>\n",
       "      <td>-139909.026614</td>\n",
       "      <td>-78893.494257</td>\n",
       "      <td>...</td>\n",
       "      <td>-667562.934524</td>\n",
       "      <td>-655215.504038</td>\n",
       "      <td>-473799.573780</td>\n",
       "      <td>-345621.989210</td>\n",
       "      <td>-9.187762e+05</td>\n",
       "      <td>-927868.499626</td>\n",
       "      <td>-679886.516377</td>\n",
       "      <td>-8.823582e+05</td>\n",
       "      <td>-474262.121982</td>\n",
       "      <td>-695698.483285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-60139.943735</td>\n",
       "      <td>-118351.239477</td>\n",
       "      <td>-228181.280629</td>\n",
       "      <td>-226276.200039</td>\n",
       "      <td>270908.204014</td>\n",
       "      <td>154758.192851</td>\n",
       "      <td>92016.023222</td>\n",
       "      <td>86065.362414</td>\n",
       "      <td>-174784.005040</td>\n",
       "      <td>-99124.084489</td>\n",
       "      <td>...</td>\n",
       "      <td>-988217.817817</td>\n",
       "      <td>-916992.338832</td>\n",
       "      <td>-753637.001609</td>\n",
       "      <td>-546489.401328</td>\n",
       "      <td>-1.036981e+06</td>\n",
       "      <td>-934869.092836</td>\n",
       "      <td>-938970.117667</td>\n",
       "      <td>-1.106399e+06</td>\n",
       "      <td>-800182.900760</td>\n",
       "      <td>-897332.083419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 196 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0              1              2              3              4    \\\n",
       "0   87022.678176  -51216.196044 -195371.505863 -252388.298108  372090.056832   \n",
       "1  118857.552212   64765.192887  -66943.170211 -125253.475061  510980.726973   \n",
       "2 -113120.619296 -100329.909243 -117236.092138  -63257.566664   87699.655400   \n",
       "3  -51897.143096  -61146.543996 -108560.424295  -80686.019435  178525.612880   \n",
       "4  -60139.943735 -118351.239477 -228181.280629 -226276.200039  270908.204014   \n",
       "\n",
       "             5              6              7              8              9    \\\n",
       "0  290238.755195  271991.646241  218795.685053   87096.609524   86361.765678   \n",
       "1  375269.217942  323695.517273  312924.577753   -6911.497818   92492.903441   \n",
       "2   15131.208777  -22576.356489  -13975.055467 -177396.925200 -125223.619520   \n",
       "3   98101.791008   48849.079713   53199.150697 -139909.026614  -78893.494257   \n",
       "4  154758.192851   92016.023222   86065.362414 -174784.005040  -99124.084489   \n",
       "\n",
       "   ...            186            187            188            189  \\\n",
       "0  ... -651953.434874 -714770.112748 -631423.743396 -693599.810348   \n",
       "1  ... -561081.908424 -506075.873333 -448764.275644 -391456.474306   \n",
       "2  ... -450443.930378 -421853.932989 -333693.022748 -204519.903296   \n",
       "3  ... -667562.934524 -655215.504038 -473799.573780 -345621.989210   \n",
       "4  ... -988217.817817 -916992.338832 -753637.001609 -546489.401328   \n",
       "\n",
       "            190            191            192           193            194  \\\n",
       "0 -7.208255e+05 -739527.228854 -471014.225482 -6.757540e+05 -350155.612260   \n",
       "1 -9.886575e+05 -958004.348328 -740390.446945 -8.927657e+05 -466337.939160   \n",
       "2 -7.137062e+05 -715494.785171 -542145.799983 -6.803446e+05 -366915.287037   \n",
       "3 -9.187762e+05 -927868.499626 -679886.516377 -8.823582e+05 -474262.121982   \n",
       "4 -1.036981e+06 -934869.092836 -938970.117667 -1.106399e+06 -800182.900760   \n",
       "\n",
       "             195  \n",
       "0 -517024.526987  \n",
       "1 -928475.702850  \n",
       "2 -572674.592498  \n",
       "3 -695698.483285  \n",
       "4 -897332.083419  \n",
       "\n",
       "[5 rows x 196 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the triangulated segment 1 and show it in a dataframe\n",
    "disp_potency_asp = data_f[\n",
    "    data_f[\"upotrough_asp\"][0][1]\n",
    "]  # time series of displacement in all 196 triangles in the 1st segment of the asperity\n",
    "\n",
    "df = pd.DataFrame(disp_potency_asp)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8329d",
   "metadata": {},
   "source": [
    "### Understanding the pre-processing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe58f38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y: [[  87022.67817585  -51216.19604374 -195371.50586285 ... -675753.9898785\n",
      "  -350155.61225988 -517024.52698663]\n",
      " [ 118857.55221177   64765.19288749  -66943.17021057 ... -892765.74462072\n",
      "  -466337.93915995 -928475.70285014]\n",
      " [-113120.61929563 -100329.90924337 -117236.09213845 ... -680344.59187365\n",
      "  -366915.28703719 -572674.59249825]\n",
      " ...\n",
      " [-549363.76936721 -443757.96941866 -121736.26484901 ...  872641.83189012\n",
      "  1224407.7626167   503931.45026343]\n",
      " [-685614.24021638 -596565.35038887 -270582.04501    ...  968835.20985008\n",
      "  1320423.6542254   667511.38181979]\n",
      " [-591740.66480993 -494054.40290934 -165083.75101661 ...  861753.46885277\n",
      "  1266822.05004271  522757.42636788]]\n",
      "Y shape: (3883, 196)\n",
      "X: [-1.51031373e+08 -1.25680553e+08 -1.07490125e+08 ...  8.05980366e+07\n",
      "  6.28760808e+07  6.00000287e+07]\n"
     ]
    }
   ],
   "source": [
    "# Understanding what the data loader is doing...\n",
    "segment = 1\n",
    "x_data = data_f[data_f[\"upotrough_asp\"][0][segment]]\n",
    "upot = dict()\n",
    "\n",
    "upot[\"seg\" + str(segment)] = x_data[()]  # assign it in the empty dictionary\n",
    "print(\"Y:\", upot[\"seg1\"])  # -> this is our Y\n",
    "print(\"Y shape:\", upot[\"seg1\"].shape)  # -> this is our Y\n",
    "\n",
    "print(\n",
    "    \"X:\", np.sum(upot[\"seg\" + str(segment)], axis=1)\n",
    ")  # -> This will be our X for some reason..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cc49077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum at 1: {} [-1.51031373e+08 -1.25680553e+08 -1.07490125e+08 ...  8.05980366e+07\n",
      "  6.28760808e+07  6.00000287e+07]\n",
      "At all other indeces...: [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I don't get the point of this section...\n",
    "Upot_raw = np.empty((n_samples, n_segments))  # empty dict for all segments\n",
    "\n",
    "Upot_raw[:, segment] = np.sum(\n",
    "    upot[\"seg\" + str(segment)], axis=1\n",
    ")  # sum over all segments and assign it to the first...\n",
    "print(\n",
    "    \"Sum at 1: {}\", Upot_raw[:, 1]\n",
    ")  # so this literally just has a time series for 1, and has 0 time series for the other segments...\n",
    "print(\"At all other indeces...:\", Upot_raw[:, 2])\n",
    "Upot_raw[:, 2] == Upot_raw[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abbf6dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3883,)\n",
      "New X shape: (3883, 1)\n"
     ]
    }
   ],
   "source": [
    "X = -Upot_raw[:, segment]  # ???? why are we doing this\n",
    "print(\"X shape:\", X.shape)\n",
    "X = X[..., np.newaxis]\n",
    "print(\"New X shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddde197e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3883, 196)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y has all the time series for each triangle in segment\n",
    "Y = upot[\"seg\" + str(segment)]\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110f9ed",
   "metadata": {},
   "source": [
    "## Pre-processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a81d0",
   "metadata": {},
   "source": [
    "#### Useful info from Michel et al. 2019\n",
    "\n",
    ">\"We use daily sampled position time series in the IGS08 reference frame from the Pacific Geodetic Array (PANGA) and the Plate Boundary Observatory (PBO) maintained by UNAVCO and processed by the Nevada Geodetic Laboratory (http://geodesy.unr.edu, last access August 2017). Most of the available continuous GPS (cGPS) stations were deployed in 2007, and we consider the time range that goes from 2007.0 to 2017.632. We use only time series with at most 40% of missing data and we exclude all the stations in the proximity (< 15 km) of volcanoes to avoid contamination by volcanic signals. We also discard station BLYN because of spurious large displacements of unknown origin that were clearly not observed at nearby stations. The final selection includes NGPS = 352 cGPS stations (Fig. 1a). We then refer all the stations to the North America reference frame using the regional block model of Schmaltze et al. (2014). The position time series are then organized in a M × T matrix Xobs, where M = 3 × NGPS is the total number of time series (East, North, and Vertical direction per each station), and T = 3883 is the total number of observed epochs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97a9f388-7c2f-4869-9e78-035c8daed0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access Cascadia and output dataframe head - need to ask Pritt about his loader\n",
    "\n",
    "# dataset = SlowEarthquakeDataset([\"cascadia\"])\n",
    "# dataset.load()\n",
    "\n",
    "# ds_exp = dataset\n",
    "# X, Y, t = ds_exp[\"X\"], ds_exp[\"Y\"], ds_exp[\"t\"]\n",
    "\n",
    "# df = pd.DataFrame(\n",
    "#     np.hstack((X, Y, t.reshape(-1, 1))),\n",
    "#     columns=[ds_exp[\"hdrs\"][\"X\"], *ds_exp[\"hdrs\"][\"Y\"], ds_exp[\"hdrs\"][\"t\"]],\n",
    "# )\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352e4dda",
   "metadata": {},
   "source": [
    "## Notes on Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45de35",
   "metadata": {},
   "source": [
    "### General notes:\n",
    "\n",
    "* We have sampled 100% of dataset (in the 3883 day time window).\n",
    "* Downsampling frequency = (Michel et al?)\n",
    "* Original 56 olumns were: ['#refs#', 'Area', 'Area_asp', 'Area_belt', 'Lg_term_slip_Rate', 'Nasp', 'Npatches', 'Npatches_asp', 'Npatches_belt', 'Nt_u', 'Nt_v', 'Upotrough_asp', 'Upotroughdot_asp', 'V0pot_asp', 'Vpotrough_asp', 'aa', 'dirs', 'fault_model_IC', 'flag_surrogate', 'id_IN_AspBorder_SSE', 'id_IN_AspBorder_SSE_dilat', 'ind_in', 'ind_in_belt', 'ind_nan', 'km2m', 'll_geom_center_ll', 'mm2m', 'shear_modulus', 'timeline_u', 'timeline_v', 'toc_start', 'upotrough', 'upotrough_asp', 'upotrough_belt', 'upotroughdot', 'upotroughdot_asp', 'upotroughdot_belt', 'urough', 'urough_asp', 'urough_belt', 'urough_mean', 'uroughdot', 'uroughdot_asp', 'uroughdot_belt', 'v0', 'v0_asp', 'v0_belt', 'v0pot', 'v0pot_asp', 'v0pot_belt', 'vpotrough', 'vpotrough_asp', 'vpotrough_belt', 'vrough', 'vrough_asp', 'vrough_belt'].\n",
    "* Loader output columns: [1, 2, ..., 196], where:\n",
    "    * Each column is a time series for the (rough?) slip displacement potency of each triangle in the chosen segment (1) of the fault asperity.\n",
    "* Pre-processing steps:\n",
    "    * Y is not pre-processed, but X is the negative sum of all Y columns... I am very confused.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79d18dd",
   "metadata": {},
   "source": [
    "### Annotated Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c61e2",
   "metadata": {},
   "source": [
    "#### Setting Experiment Parameter\n",
    "From _params.py_: \n",
    "\n",
    "```python\n",
    "elif exp == \"cascadia\":\n",
    "        parameters = {\n",
    "            \"t0\": 2000.0,           # Starting time window loaded - Note: raw data min = 0\n",
    "            \"tend\": 2030.0,         # Ending time window loaded - Note: raw data max = 5285.9\n",
    "            \"Nheaders\": None,       # Header that np array starts with in import_data\n",
    "            \"dir_data\": \"gtc_quakes_data/slowquakes/\",\n",
    "            \"case_study\": \"Micheletal2019a/dynamical_system_variables_rough\",\n",
    "            \"data_type\": \"nature\",\n",
    "            \"struct_type\": None,\n",
    "            \"file_format\": \"mat\",\n",
    "            \"downsample_factor\": 1, # No downsampling (in != 1, no code has been written for it)\n",
    "            \"vl\": 40e-3,            # Loading velocity\n",
    "            \"segment\": 1,           # Segment in asperity to select\n",
    "            \"obs_unit\": r\"m$^3$\",\n",
    "            \"time_unit\": \"yr\",\n",
    "        }\n",
    "\n",
    "        [...] # Assigns new params for obs and time labels with units\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026af63",
   "metadata": {},
   "source": [
    "#### Loading and Pre-processing\n",
    "Note: load_data() takes in the parameters defined above, loads and processes the data, and outputs it into X, Y, t, dt, vl.\n",
    "\n",
    "Relevant parts from _load.py_: \n",
    "\n",
    "```python\n",
    "def load_data(exp, dirs, params):\n",
    "\n",
    "    elif params[\"data_type\"] == \"nature\":\n",
    "        [...] # load the file using h5py\n",
    "\n",
    "        # Select the \"upotrough_asp\" and \"timeline_u\" columns in data\n",
    "        # Note \"upotrough_asp\" shape = 14x1, with each cell having shape = n x 3883 (n differs per cell)\n",
    "        # So I would assume all 14 are different time series - are they the neighbouring stations like Adriano was saying?\n",
    "        upotrough_asp_ref = f.get(\"upotrough_asp\") \n",
    "        t_data = f.get(\"timeline_u\")        # shape = 1x3883, observed eppchs\n",
    "        t = t_data[()][:, 0]                # t_data[()] gets entire dataset; then select rows from the first column\n",
    "\n",
    "        dt = t[1] - t[0]                    # look at time step in data (1 day)\n",
    "\n",
    "        n_samples = t.shape[0]              # n of samples\n",
    "        n_segments = upotrough_asp_ref.shape[1] # diplacement potency of asperity (rough??)\n",
    "\n",
    "        upot = dict()                       # diplacement potency dict for desired dict\n",
    "        Upot_raw = np.empty((n_samples, n_segments))    # raw diplacement potency for all segments (though most remain empty?? What is the point of this...)\n",
    "\n",
    "        # Choose desired triangulated segment of the sperity \n",
    "        if params[\"segment\"] is None:\n",
    "            segment = 0\n",
    "        else:\n",
    "            segment = params[\"segment\"]                 # Parameter set at 1 in the loader\n",
    "\n",
    "        x_data = f[upotrough_asp_ref[0][segment]]       # select the time series for the desired segment\n",
    "        upot[\"seg\" + str(segment)] = x_data[()]         # assign it in the empty dictionary\n",
    "        Upot_raw[:, segment] = np.sum(upot[\"seg\" + str(segment)], axis=1) # sum over all triangles... why not average? Why not just access Upotrough_asp instead? - but actually they are different. Questions for Adriano.\n",
    "\n",
    "        Y = upot[\"seg\" + str(segment)]  # output all 196 triangles as Y time series\n",
    "        X = -Upot_raw[:, segment]       # why??? why is our X a sum of all Ys...\n",
    "        X = X[..., np.newaxis]          # reshape from (3883,) to (3883, 1)\n",
    "\n",
    "        vl = params[\"vl\"]\n",
    "        [...] #---- Estimate loading velocity from loading displacenment if not present, but in Cascadia vl=40e-3\n",
    "            \n",
    "    \n",
    "    return X, Y, t, dt, vl # note we read the first 3 in as out 6 column dataset [X, Y, t]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

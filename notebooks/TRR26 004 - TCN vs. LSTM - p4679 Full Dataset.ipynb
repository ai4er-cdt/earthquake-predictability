{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN vs LSTM - Using Full p4679 Dataset\n",
    "\n",
    "Objectives\n",
    "\n",
    "- Extract full p4679 dataset. Note this requires dealing with large files. Load in full file, but only display smoothed data?\n",
    "- Think about pre-processing. Try to keep as much info as possible, while filtering out noise.. how?\n",
    "- Create train/test split\n",
    "- Create standardised way to do feature creation. Create features for shear stress, derivative and variance.\n",
    "- Create simple LSTM and TCN models\n",
    "- Create information training loop\n",
    "- Create test procedure \n",
    "- Features - instead of raw ss, put in smoothed, alongside variance and/or \"noise\" = raw - smoothed?\n",
    "- Don't try and predict full cycle, just 30 or so steps - better aligns with theory.. also probably impossible to do whole cycle starting from 0\n",
    "- Segment by cycle - about 230.. so ~150 to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Directories\n",
    "\n",
    "jasmin = True  # Set to True if running on JASMIN, False if on local machine\n",
    "jasmin_user_name = \"trr26\"\n",
    "\n",
    "if jasmin:\n",
    "    MAIN_DIR = f\"/gws/nopw/j04/ai4er/users/{jasmin_user_name}/earthquake-predictability\"\n",
    "    DATA_DIR = f\"{MAIN_DIR}/data/gtc_quakes_data\"\n",
    "\n",
    "else:  # update directory names to match your local machine\n",
    "    MAIN_DIR = f\"/home/tom-ratsakatika/VSCode/earthquake-predictability\"\n",
    "    DATA_DIR = f\"{MAIN_DIR}/data_local\"\n",
    "\n",
    "p4679_FILE_PATH = f\"{DATA_DIR}/labquakes/Marone/p4679/p4679.txt\"\n",
    "\n",
    "# Imports\n",
    "\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from math import sqrt\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from IPython.display import Image, display\n",
    "from scipy.io import loadmat\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(MAIN_DIR)\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA Availability\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda\"\n",
    "    map_location = None\n",
    "    print(f\"Total number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "    map_location = \"cpu\"\n",
    "    print(\"No GPU available.\")\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file located at p4581_FILE_PATH in read mode\n",
    "with open(p4679_FILE_PATH, \"r\") as file:\n",
    "    # Read the file as a CSV using pandas, considering whitespace as delimiter\n",
    "    # Skip the first 4 rows as they do not contain relevant data\n",
    "    df = pd.read_csv(file, skiprows=1)\n",
    "\n",
    "# Rename the columns of the DataFrame to align with the variable names used by Adriano's data loader\n",
    "df.columns = [\n",
    "    \"id\",\n",
    "    \"lp_disp\",\n",
    "    \"shr_stress\",\n",
    "    \"nor_disp\",\n",
    "    \"nor_stress\",\n",
    "    \"time\",\n",
    "    \"mu\",\n",
    "    \"layer_thick\",\n",
    "    \"ec_disp\",\n",
    "]\n",
    "\n",
    "# Drop the 'id' column as it's not needed for further analysis\n",
    "df = df.drop(\n",
    "    [\n",
    "        \"id\",\n",
    "        \"lp_disp\",\n",
    "        \"nor_disp\",\n",
    "        \"mu\",\n",
    "        \"nor_stress\",\n",
    "        \"layer_thick\",\n",
    "        \"ec_disp\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Reorder the DataFrame columns to bring 'time' to the front\n",
    "df = df[[\"time\"] + [col for col in df.columns if col != \"time\"]]\n",
    "\n",
    "# Define start and end times for the data selection\n",
    "start_time = 4233.364\n",
    "end_time = 5159.292\n",
    "\n",
    "df = df[(df[\"time\"] >= start_time) & (df[\"time\"] <= end_time)]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "source = ColumnDataSource(df)\n",
    "p = figure(width=800, height=400)\n",
    "p.line(x=\"time\", y=\"shr_stress\", source=source)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peak_indices(data):\n",
    "    threshold = 100\n",
    "    peak_indices = [0]\n",
    "    for i in range(threshold, len(data) - threshold):\n",
    "        if data[i] > max(data[i - threshold : i]) and data[i] >= max(\n",
    "            data[i + 1 : i + threshold]\n",
    "        ):\n",
    "            peak_indices.append(i)\n",
    "    return peak_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the find_peak_indices function to get the peak and trough indices\n",
    "peak_indices = find_peak_indices(df[\"shr_stress\"])\n",
    "trough_indices = find_peak_indices(-df[\"shr_stress\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"peaks\"] = np.where(df.index.isin(peak_indices), df[\"shr_stress\"], np.nan)\n",
    "df[\"troughs\"] = np.where(\n",
    "    df.index.isin(trough_indices), df[\"shr_stress\"], np.nan\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time since last peak\n",
    "last_peak_time = None\n",
    "for i in range(len(df)):\n",
    "    if pd.notna(df.loc[i, \"peaks\"]):\n",
    "        last_peak_time = df.loc[i, \"time\"]\n",
    "    df.loc[i, \"time_since_last_peak\"] = (\n",
    "        np.nan\n",
    "        if last_peak_time is None\n",
    "        else df.loc[i, \"time\"] - last_peak_time\n",
    "    )\n",
    "\n",
    "# Calculate time since last trough\n",
    "last_trough_time = None\n",
    "for i in range(len(df)):\n",
    "    if pd.notna(df.loc[i, \"troughs\"]):\n",
    "        last_trough_time = df.loc[i, \"time\"]\n",
    "    df.loc[i, \"time_since_last_trough\"] = (\n",
    "        np.nan\n",
    "        if last_trough_time is None\n",
    "        else df.loc[i, \"time\"] - last_trough_time\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "\n",
    "source = ColumnDataSource(df)\n",
    "\n",
    "p = figure(\n",
    "    width=1200,\n",
    "    height=400,\n",
    "    x_axis_label=\"Time\",\n",
    "    y_axis_label=\"Shear Stress\",\n",
    "    title=\"Line Graph of Shear Stress with Peaks and Troughs Marked\",\n",
    ")\n",
    "p.line(x=\"time\", y=\"shr_stress\", source=source)\n",
    "\n",
    "\n",
    "# Add peaks with red x marks\n",
    "p.cross(x=\"time\", y=\"peaks\", source=source, color=\"red\", size=8)\n",
    "\n",
    "# Add troughs with green x marks\n",
    "p.cross(x=\"time\", y=\"troughs\", source=source, color=\"green\", size=8)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "\n",
    "source = ColumnDataSource(df)\n",
    "\n",
    "p = figure(\n",
    "    width=1200,\n",
    "    height=400,\n",
    "    x_axis_label=\"Time\",\n",
    "    y_axis_label=\"Shear Stress\",\n",
    "    title=\"Line Graph of Shear Stress with Peaks and Troughs Marked\",\n",
    ")\n",
    "p.line(x=\"time\", y=\"time_since_last_peak\", source=source, line_color=\"red\")\n",
    "p.line(x=\"time\", y=\"time_since_last_trough\", source=source, line_color=\"green\")\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps\n",
    "- Try work out bug with identification of peaks\n",
    "- Calculate derivative, and turning point\n",
    "- Maybe do min-max before and slight smoothing?\n",
    "- think about what defines a tipping point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"shr_stress_mv_avg\"] = (\n",
    "    df[\"shr_stress\"]\n",
    "    .rolling(30)\n",
    "    .apply(lambda w: scipy.stats.trim_mean(w, 0.05))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "\n",
    "source = ColumnDataSource(df)\n",
    "\n",
    "p = figure(\n",
    "    width=1200,\n",
    "    height=400,\n",
    "    x_axis_label=\"Time\",\n",
    "    y_axis_label=\"Shear Stress\",\n",
    "    title=\"Line Graph of Shear Stress with Peaks and Troughs Marked\",\n",
    ")\n",
    "p.line(x=\"time\", y=\"shr_stress\", source=source, line_color=\"blue\")\n",
    "p.line(x=\"time\", y=\"shr_stress_mv_avg\", source=source, line_color=\"green\")\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segments = np.split(df, trough_indices)\n",
    "df_segments[228].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The metric needs to work even if there isn't a failure if we have random segments that don't have a drop\n",
    "- Do it based on 2nd derivative .. weight MSE based on magnitude of the second derivative\n",
    "- So do derivative-weighted RMSE\n",
    "- Note that derivative for input needs to be trailing, but for loss function doesn't have to be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_lengths = [len(segment) for segment in df_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(segment_lengths)\n",
    "plt.xlabel(\"Segment Index\")\n",
    "plt.ylabel(\"Segment Length\")\n",
    "plt.title(\"Line Chart of Segment Lengths\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
